import mlflow
import mlflow.sklearn
import pandas as pd
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# MLflow ì„¤ì •
mlflow.set_tracking_uri(uri="http://localhost:8080")
mlflow.set_experiment("Model Comparison Analysis")

def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):
    """MLflowì™€ ì‚¬ì´í‚·ëŸ°ì„ í•¨ê»˜ ì‚¬ìš©í•œ ëª¨ë¸ í‰ê°€ í•¨ìˆ˜"""
    
    with mlflow.start_run(run_name=model_name):
        # 1ï¸âƒ£ ëª¨ë¸ íŒŒë¼ë¯¸í„° ê¸°ë¡ (MLflow)
        mlflow.log_params(model.get_params())
        
        # 2ï¸âƒ£ ëª¨ë¸ í•™ìŠµ (ì‚¬ì´í‚·ëŸ°)
        model.fit(X_train, y_train)
        
        # 3ï¸âƒ£ ì˜ˆì¸¡ ë° í‰ê°€ (ì‚¬ì´í‚·ëŸ°)
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
        
        # 4ï¸âƒ£ ë©”íŠ¸ë¦­ ê³„ì‚° (ì‚¬ì´í‚·ëŸ°)
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        # 5ï¸âƒ£ Cross-validation ì ìˆ˜ (ì‚¬ì´í‚·ëŸ°)
        cv_scores = cross_val_score(model, X_train, y_train, cv=5)
        cv_mean = cv_scores.mean()
        cv_std = cv_scores.std()
        
        # 6ï¸âƒ£ ë©”íŠ¸ë¦­ ê¸°ë¡ (MLflow)
        mlflow.log_metric("accuracy", accuracy)
        mlflow.log_metric("precision", precision)
        mlflow.log_metric("recall", recall)
        mlflow.log_metric("f1_score", f1)
        mlflow.log_metric("cv_score_mean", cv_mean)
        mlflow.log_metric("cv_score_std", cv_std)
        
        # 7ï¸âƒ£ íƒœê·¸ ì„¤ì • (MLflow)
        mlflow.set_tag("model_type", model_name)
        mlflow.set_tag("dataset", "iris")
        
        # 8ï¸âƒ£ ëª¨ë¸ ì €ì¥ (MLflow)
        mlflow.sklearn.log_model(
            sk_model=model,
            name="model",
            registered_model_name=f"iris_{model_name.lower()}"
        )
        
        # 9ï¸âƒ£ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ì €ì¥ (MLflow + ì‚¬ì´í‚·ëŸ°)
        report = classification_report(y_test, y_pred, output_dict=True)
        report_df = pd.DataFrame(report).transpose()
        report_df.to_csv("classification_report.csv")
        mlflow.log_artifact("classification_report.csv")
        
        print(f"\n=== {model_name} ê²°ê³¼ ===")
        print(f"ì •í™•ë„: {accuracy:.4f}")
        print(f"ì •ë°€ë„: {precision:.4f}")
        print(f"ì¬í˜„ìœ¨: {recall:.4f}")
        print(f"F1 ì ìˆ˜: {f1:.4f}")
        print(f"CV ì ìˆ˜: {cv_mean:.4f} (Â±{cv_std:.4f})")
        
        return {
            'model_name': model_name,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'cv_mean': cv_mean,
            'cv_std': cv_std
        }

def main():
    """ë©”ì¸ ì‹¤í—˜ í•¨ìˆ˜ - MLflowì™€ ì‚¬ì´í‚·ëŸ° êµì°¨ ì‚¬ìš©"""
    
    # 1ï¸âƒ£ ë°ì´í„° ì¤€ë¹„ (ì‚¬ì´í‚·ëŸ°)
    print("ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
    X, y = datasets.load_iris(return_X_y=True)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # 2ï¸âƒ£ ì—¬ëŸ¬ ëª¨ë¸ ì •ì˜ (ì‚¬ì´í‚·ëŸ°)
    models = {
        "LogisticRegression": LogisticRegression(random_state=42, max_iter=1000),
        "RandomForest": RandomForestClassifier(random_state=42, n_estimators=100),
        "SVM": SVC(random_state=42, probability=True)
    }
    
    # 3ï¸âƒ£ ê° ëª¨ë¸ì— ëŒ€í•´ ì‹¤í—˜ ì‹¤í–‰ (MLflow + ì‚¬ì´í‚·ëŸ°)
    results = []
    print("\nğŸ”¬ ëª¨ë¸ ì‹¤í—˜ ì‹œì‘...")
    
    for model_name, model in models.items():
        result = evaluate_model(model, X_train, X_test, y_train, y_test, model_name)
        results.append(result)
    
    # 4ï¸âƒ£ ê²°ê³¼ ë¹„êµ ë¶„ì„ (pandas + MLflow)
    results_df = pd.DataFrame(results)
    print("\nğŸ“ˆ ëª¨ë¸ ë¹„êµ ê²°ê³¼:")
    print(results_df.to_string(index=False))
    
    # 5ï¸âƒ£ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
    best_model = results_df.loc[results_df['accuracy'].idxmax()]
    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['model_name']} (ì •í™•ë„: {best_model['accuracy']:.4f})")
    
    # 6ï¸âƒ£ ë¹„êµ ì°¨íŠ¸ ìƒì„± ë° ì €ì¥ (MLflow)
    with mlflow.start_run(run_name="Model_Comparison_Summary"):
        # ê²°ê³¼ í…Œì´ë¸” ì €ì¥
        results_df.to_csv("model_comparison.csv", index=False)
        mlflow.log_artifact("model_comparison.csv")
        
        # ë¹„êµ ì°¨íŠ¸ ìƒì„±
        plt.figure(figsize=(12, 8))
        
        # ì„œë¸Œí”Œë¡¯ ìƒì„±
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # ì •í™•ë„ ë¹„êµ
        axes[0, 0].bar(results_df['model_name'], results_df['accuracy'])
        axes[0, 0].set_title('ëª¨ë¸ë³„ ì •í™•ë„ ë¹„êµ')
        axes[0, 0].set_ylabel('ì •í™•ë„')
        
        # F1 ì ìˆ˜ ë¹„êµ
        axes[0, 1].bar(results_df['model_name'], results_df['f1_score'])
        axes[0, 1].set_title('ëª¨ë¸ë³„ F1 ì ìˆ˜ ë¹„êµ')
        axes[0, 1].set_ylabel('F1 ì ìˆ˜')
        
        # CV ì ìˆ˜ ë¹„êµ (ì—ëŸ¬ë°” í¬í•¨)
        axes[1, 0].bar(results_df['model_name'], results_df['cv_mean'], 
                       yerr=results_df['cv_std'], capsize=5)
        axes[1, 0].set_title('ëª¨ë¸ë³„ Cross-Validation ì ìˆ˜')
        axes[1, 0].set_ylabel('CV ì ìˆ˜')
        
        # ì¢…í•© ë©”íŠ¸ë¦­ íˆíŠ¸ë§µ
        metrics_data = results_df[['accuracy', 'precision', 'recall', 'f1_score']].T
        metrics_data.columns = results_df['model_name']
        sns.heatmap(metrics_data, annot=True, fmt='.3f', ax=axes[1, 1])
        axes[1, 1].set_title('ëª¨ë¸ë³„ ë©”íŠ¸ë¦­ íˆíŠ¸ë§µ')
        
        plt.tight_layout()
        plt.savefig("model_comparison_charts.png", dpi=300, bbox_inches='tight')
        mlflow.log_artifact("model_comparison_charts.png")
        plt.close()
        
        # ìš”ì•½ ë©”íŠ¸ë¦­ ê¸°ë¡
        mlflow.log_metric("best_accuracy", best_model['accuracy'])
        mlflow.log_metric("model_count", len(models))
        mlflow.set_tag("experiment_type", "model_comparison")
        mlflow.set_tag("best_model", best_model['model_name'])
    
    print(f"\nâœ… ì‹¤í—˜ ì™„ë£Œ! MLflow UIì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”: http://localhost:8080")

if __name__ == "__main__":
    main() 